{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "communist-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe2684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fdafa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>nuniq_cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365271984</td>\n",
       "      <td>winter_sport</td>\n",
       "      <td>[волшебные, фото, виктория, поплавская, евгени...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503385563</td>\n",
       "      <td>extreme</td>\n",
       "      <td>[возвращение, в, подземелье, треша, 33, эйфори...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146016084</td>\n",
       "      <td>football</td>\n",
       "      <td>[лучшие, чешские, вратари, –, доминик, доминат...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>933865449</td>\n",
       "      <td>boardgames</td>\n",
       "      <td>[rtokenoid, warhammer40k, валрак, решил, нас, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>713550145</td>\n",
       "      <td>hockey</td>\n",
       "      <td>[шестеркин, затаскивает, рейнджерс, в, финал, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38735</th>\n",
       "      <td>910636962</td>\n",
       "      <td>autosport</td>\n",
       "      <td>[8, битная, буря, снова, накрыла, пикселями, а...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38736</th>\n",
       "      <td>669736851</td>\n",
       "      <td>autosport</td>\n",
       "      <td>[ира, сидоркова, объясняет, как, сказалась, на...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38737</th>\n",
       "      <td>558919241</td>\n",
       "      <td>tennis</td>\n",
       "      <td>[24, я, ракетка, мира, хорват, марин, чилич, о...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38738</th>\n",
       "      <td>776944963</td>\n",
       "      <td>volleyball</td>\n",
       "      <td>[стал, известен, календарь, мужской, сборной, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38739</th>\n",
       "      <td>577334983</td>\n",
       "      <td>hockey</td>\n",
       "      <td>[первенство, вхл., первый, этап, динамо, алтай...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37803 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             oid      category  \\\n",
       "0      365271984  winter_sport   \n",
       "1      503385563       extreme   \n",
       "2      146016084      football   \n",
       "3      933865449    boardgames   \n",
       "4      713550145        hockey   \n",
       "...          ...           ...   \n",
       "38735  910636962     autosport   \n",
       "38736  669736851     autosport   \n",
       "38737  558919241        tennis   \n",
       "38738  776944963    volleyball   \n",
       "38739  577334983        hockey   \n",
       "\n",
       "                                                    text  nuniq_cats  \n",
       "0      [волшебные, фото, виктория, поплавская, евгени...           1  \n",
       "1      [возвращение, в, подземелье, треша, 33, эйфори...           1  \n",
       "2      [лучшие, чешские, вратари, –, доминик, доминат...           1  \n",
       "3      [rtokenoid, warhammer40k, валрак, решил, нас, ...           1  \n",
       "4      [шестеркин, затаскивает, рейнджерс, в, финал, ...           1  \n",
       "...                                                  ...         ...  \n",
       "38735  [8, битная, буря, снова, накрыла, пикселями, а...           1  \n",
       "38736  [ира, сидоркова, объясняет, как, сказалась, на...           1  \n",
       "38737  [24, я, ракетка, мира, хорват, марин, чилич, о...           1  \n",
       "38738  [стал, известен, календарь, мужской, сборной, ...           1  \n",
       "38739  [первенство, вхл., первый, этап, динамо, алтай...           1  \n",
       "\n",
       "[37803 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Удаляем тексты, которые распределены больше, чем на одну категорию\n",
    "df_train['nuniq_cats'] = df_train.groupby('text').category.transform(lambda x: x.nunique())\n",
    "df_train = df_train[df_train.nuniq_cats == 1]\n",
    "#Удаляем дубликаты\n",
    "df_train = df_train.drop_duplicates()\n",
    "#Переводим колонку text в удобный для нас формат\n",
    "df_train.text = df_train.text.str.lower().str.split()\n",
    "#Удаляем сообщества, у которых меньше 5 постов\n",
    "df_train = df_train[df_train.groupby('oid')['text'].transform('count') >= 5]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ahead-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seq = df_train.text.values\n",
    "label_seq = df_train.category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stopped-click",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 81958),\n",
       " ('и', 58581),\n",
       " ('на', 42206),\n",
       " ('с', 25921),\n",
       " ('не', 24943),\n",
       " ('33', 21100),\n",
       " ('что', 20830),\n",
       " ('по', 15038),\n",
       " ('я', 12967),\n",
       " ('за', 12060)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2cnt = Counter([token for sentence in token_seq for token in sentence])\n",
    "token2cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаляем токены, содержащие английские буквы(это в основном \"мусор\")\n",
    "def match(text, alphabet=set('abcdefghijklmnopqrstuvwxyz')):\n",
    "    return not alphabet.isdisjoint(text.lower())\n",
    "\n",
    "keys = list(token2cnt.keys())\n",
    "\n",
    "for key in keys:\n",
    "    if match(key):\n",
    "        del token2cnt[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ignored-corps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов в тренировочном датасете: 166842\n",
      "Количество слов встречающихся меньше 3-х раз в тренировочном датасете: 107060\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество уникальных слов в тренировочном датасете: {len(token2cnt)}\")\n",
    "print(f\"Количество слов встречающихся меньше 3-х раз в тренировочном датасете: {len([token for token, cnt in token2cnt.items() if cnt <= 2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "empirical-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token2idx(\n",
    "    token2cnt,\n",
    "    min_count,\n",
    "):\n",
    "\n",
    "    token2idx = {}\n",
    "\n",
    "    token2idx['<PAD>'] = 0\n",
    "    token2idx['<UNK>'] = 1    \n",
    "    i = 2\n",
    "    for token, cnt in token2cnt.items():\n",
    "        if cnt >= min_count:\n",
    "            token2idx[token] = i\n",
    "            i += 1\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "thrown-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = get_token2idx(token2cnt, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dangerous-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label2idx(label_set):\n",
    "\n",
    "    label2idx = {}\n",
    "\n",
    "    i = 0\n",
    "    for label in label_set:\n",
    "        label2idx[label] = i\n",
    "        i += 1\n",
    "    return label2idx\n",
    "\n",
    "label_set = df_train.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fluid-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = get_label2idx(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "amber-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>             0\n",
      "<UNK>             1\n",
      "волшебные         2\n",
      "фото              3\n",
      "виктория          4\n",
      "евгениямедведева  5\n",
      "возвращение       6\n",
      "в                 7\n",
      "подземелье        8\n",
      "33                9\n"
     ]
    }
   ],
   "source": [
    "for token, idx in list(token2idx.items())[:10]:\n",
    "    print(\"{:<17}\".format(token), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "healthy-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winter_sport    0\n",
      "extreme         1\n",
      "football        2\n",
      "boardgames      3\n",
      "hockey          4\n",
      "esport          5\n",
      "athletics       6\n",
      "motosport       7\n",
      "basketball      8\n",
      "tennis          9\n",
      "autosport       10\n",
      "martial_arts    11\n",
      "volleyball      12\n"
     ]
    }
   ],
   "source": [
    "for label, idx in label2idx.items():\n",
    "    print(\"{:<15}\".format(label), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "virgin-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq,\n",
    "        label_seq,\n",
    "        token2idx,\n",
    "        label2idx,\n",
    "    ):\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
    "        self.label_seq = self.process_labels(label_seq, label2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx,\n",
    "    ):\n",
    "        label = torch.LongTensor([self.label_seq[idx]])\n",
    "        return torch.LongTensor(self.token_seq[idx]), label\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_tokens(\n",
    "        tokens,\n",
    "        token2idx,\n",
    "        unk = \"<UNK>\",\n",
    "    ):\n",
    "        idxs = []\n",
    "        for tkn in tokens:\n",
    "            if tkn in token2idx.keys():\n",
    "                idxs.append(token2idx[tkn])\n",
    "            else:\n",
    "                idxs.append(token2idx[unk])\n",
    "        return idxs\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels,\n",
    "        label2idx,\n",
    "    ):\n",
    "        idxs = []\n",
    "        for lbl in labels:\n",
    "            idxs.append(label2idx[lbl])\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "intensive-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32118 5685\n"
     ]
    }
   ],
   "source": [
    "oids = df_train.oid.unique()\n",
    "train_oids, val_oids = train_test_split(oids, test_size=0.15, shuffle=True, random_state=42)\n",
    "df_val = df_train[df_train.oid.isin(val_oids)]\n",
    "df_train = df_train[df_train.oid.isin(train_oids)]\n",
    "\n",
    "train_token_seq = df_train.text.values\n",
    "train_label_seq = df_train.category.values\n",
    "val_token_seq = df_val.text.values\n",
    "val_label_seq = df_val.category.values\n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx\n",
    ")\n",
    "val_dataset = MyDataset(\n",
    "    token_seq=val_token_seq,\n",
    "    label_seq=val_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx\n",
    ")\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "removed-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 3, 4, 1, 5, 1]), tensor([0]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9a2857fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 33,  16, 582, 583,   1, 584, 585,  29,  29,  29,  75,  16,  12, 586,\n",
       "         397, 587,   1, 588, 589, 590, 591, 592, 593, 301, 594, 180, 595, 510,\n",
       "         596, 301, 597, 598, 599,  12, 600, 591, 467, 601, 602, 603, 168,  27,\n",
       "         604, 605, 606,   1, 607, 301, 608, 591, 467, 609, 610,  27, 611, 612,\n",
       "         613, 614,   1, 591, 467, 615, 616, 105, 168, 617,  91,  27, 618, 619,\n",
       "         591, 591, 620, 621, 294, 196, 622, 623, 624, 231, 625, 626, 627,  33,\n",
       "         336, 628, 629, 630, 631, 259, 632, 633, 298, 634,   1,  30, 164, 635,\n",
       "         636,   1, 336,   1,   7, 637,   7, 166, 638, 639,  12, 640, 641, 178,\n",
       "         642, 196, 231, 643,   1,   1,  23, 644, 334,  27,   1, 645,  91, 507,\n",
       "         646, 647, 648, 591,   1, 649, 650, 651, 486,  27, 652,   1,  30, 653,\n",
       "         654, 655, 161, 656,  27, 657,  53,   1, 658, 659, 660, 661, 662,   1,\n",
       "         591, 663, 664,  73, 665, 454, 666, 667, 668, 669, 591,   1, 670,   1,\n",
       "           1,   1,   1, 671, 672, 673, 674, 675]),\n",
       " tensor([7]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "refined-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Collator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_padding_value,\n",
    "    ):\n",
    "        self.token_padding_value = token_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch,\n",
    "    ):\n",
    "\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        tokens = torch.nn.utils.rnn.pad_sequence(tokens, padding_value = self.token_padding_value, \n",
    "                                                 batch_first=True)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, padding_value = self.token_padding_value, \n",
    "                                                 batch_first=True)\n",
    "\n",
    "        return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "certified-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = My_Collator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fresh-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=10,  \n",
    "    shuffle=False, \n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-walker",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "parental-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, n_classes: int, dropout: float = 0.4):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, n_classes)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = output.mean(dim=0)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "powered-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#model = TransformerModel(\n",
    "#    ntoken=len(token2idx),\n",
    "#    d_model=128,\n",
    "#    nhead=8,\n",
    "#    d_hid=256,\n",
    "#    nlayers=1,\n",
    "#    n_classes=len(label2idx)\n",
    "#).to(device)\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntoken=len(token2idx),\n",
    "    d_model=96,\n",
    "    nhead=8,\n",
    "    d_hid=96,\n",
    "    nlayers=1,\n",
    "    n_classes=len(label2idx)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "lovely-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay = 0.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay = 0.0)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "adjusted-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_accuracy(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum()\n",
    "    return 2 * correct / len(y_true) - 1\n",
    "\n",
    "def train_epoch(model, device, dataloader, loss_fn, optimizer):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    Output = None\n",
    "    Labels = None\n",
    "\n",
    "    for tokens, labels in tqdm(dataloader):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        tokens = tokens.transpose(0, 1)\n",
    "        bptt = tokens.shape[0]\n",
    "        src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(tokens, src_mask)\n",
    "        \n",
    "        loss = loss_fn(output, labels[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = output.cpu().detach().argmax(dim=1).squeeze().numpy()\n",
    "        labels = labels.squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        if Output is None:\n",
    "            Output = output\n",
    "            Labels = labels\n",
    "        else:\n",
    "            Output = np.concatenate((Output, output))\n",
    "            Labels = np.concatenate((Labels, labels))\n",
    "        \n",
    "    pen_acc = penalty_accuracy(np.array(Labels), np.array(Output)) \n",
    "    return train_loss / len(dataloader), pen_acc\n",
    "  \n",
    "def valid_epoch(model, device, dataloader, loss_fn, return_output=False):\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    Output = []\n",
    "    Labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            \n",
    "            images = images.transpose(0, 1)\n",
    "            bptt = images.shape[0]\n",
    "            src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "            \n",
    "            output = model(images, src_mask).cpu()\n",
    "            \n",
    "            loss = loss_fn(output, labels[:, 0])\n",
    "            val_loss += loss.item()\n",
    "            output = output.argmax(dim=1).squeeze().numpy()\n",
    "            labels = labels.squeeze().numpy()\n",
    "\n",
    "            if Output is None:\n",
    "                Output = output\n",
    "                Labels = labels\n",
    "            else:\n",
    "                Output = np.concatenate((Output, output))\n",
    "                Labels = np.concatenate((Labels, labels))\n",
    "     \n",
    "    pen_acc = penalty_accuracy(np.array(Labels), np.array(Output)) \n",
    "    if return_output:\n",
    "        return val_loss / len(dataloader), pen_acc, Output, Labels\n",
    "    return val_loss / len(dataloader), pen_acc\n",
    "\n",
    "def fit(model, trainloader, validloader, optimizer, criterion, epochs):\n",
    "    best_pa = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss, train_pa= train_epoch(model, device, trainloader, criterion, optimizer)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_pa = valid_epoch(model, device, validloader, criterion)\n",
    "        \n",
    "        if val_pa > best_pa:\n",
    "            best_pa = val_pa\n",
    "            torch.save(model, 'best_model2.pt')\n",
    "\n",
    "        print(\"Epoch:{}/{} \\n\".format(epoch + 1, epochs),\n",
    "              \"Training: Loss:{:.5f}, pen_acc:{:.5f} \\n\".format(train_loss, \n",
    "                                                                train_pa),\n",
    "              \"Validation: Loss:{:.5f}, pen_acc:{:.5f}\".format(val_loss,\n",
    "                                                               val_pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "happy-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 16059/16059 [01:17<00:00, 206.33it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:02<00:00, 264.17it/s]\n",
      "  0%|                                       | 24/16059 [00:00<01:07, 235.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/18 \n",
      " Training: Loss:1.50831, pen_acc:0.04135 \n",
      " Validation: Loss:1.22692, pen_acc:0.31680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [05:38<00:00, 47.49it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.62it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:23, 36.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/18 \n",
      " Training: Loss:0.94196, pen_acc:0.43415 \n",
      " Validation: Loss:1.18759, pen_acc:0.42058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.52it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.38it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:26, 35.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/18 \n",
      " Training: Loss:0.71673, pen_acc:0.58086 \n",
      " Validation: Loss:1.04922, pen_acc:0.48813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.54it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.77it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:12, 43.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/18 \n",
      " Training: Loss:0.57891, pen_acc:0.65826 \n",
      " Validation: Loss:1.07229, pen_acc:0.53492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.52it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.42it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:11, 37.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/18 \n",
      " Training: Loss:0.48792, pen_acc:0.71929 \n",
      " Validation: Loss:1.23064, pen_acc:0.55286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.49it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.41it/s]\n",
      "  0%|                                         | 4/16059 [00:00<06:46, 39.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/18 \n",
      " Training: Loss:0.42111, pen_acc:0.75659 \n",
      " Validation: Loss:1.08649, pen_acc:0.54477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.55it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 166.07it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:55, 33.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/18 \n",
      " Training: Loss:0.35778, pen_acc:0.79339 \n",
      " Validation: Loss:1.06689, pen_acc:0.56588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.53it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.56it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:21, 36.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/18 \n",
      " Training: Loss:0.30748, pen_acc:0.82315 \n",
      " Validation: Loss:1.23249, pen_acc:0.57678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.50it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.69it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:15, 36.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/18 \n",
      " Training: Loss:0.27320, pen_acc:0.84495 \n",
      " Validation: Loss:1.14182, pen_acc:0.58347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.55it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.60it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:18, 42.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/18 \n",
      " Training: Loss:0.24729, pen_acc:0.85777 \n",
      " Validation: Loss:1.75055, pen_acc:0.57643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.54it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.60it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:20, 42.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/18 \n",
      " Training: Loss:0.22543, pen_acc:0.86967 \n",
      " Validation: Loss:1.51584, pen_acc:0.58065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.52it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.58it/s]\n",
      "  0%|                                         | 4/16059 [00:00<07:30, 35.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/18 \n",
      " Training: Loss:0.21093, pen_acc:0.88044 \n",
      " Validation: Loss:1.22803, pen_acc:0.59015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.54it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.56it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:25, 41.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/18 \n",
      " Training: Loss:0.18663, pen_acc:0.89246 \n",
      " Validation: Loss:1.54396, pen_acc:0.57889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.53it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.71it/s]\n",
      "  0%|                                         | 4/16059 [00:00<06:49, 39.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/18 \n",
      " Training: Loss:0.17341, pen_acc:0.90049 \n",
      " Validation: Loss:1.94482, pen_acc:0.55462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.50it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.08it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:14, 42.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/18 \n",
      " Training: Loss:0.15433, pen_acc:0.91656 \n",
      " Validation: Loss:1.92854, pen_acc:0.57537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.50it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.22it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:14, 42.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/18 \n",
      " Training: Loss:0.14899, pen_acc:0.91544 \n",
      " Validation: Loss:2.51153, pen_acc:0.57608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.50it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.56it/s]\n",
      "  0%|                                         | 5/16059 [00:00<06:04, 44.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/18 \n",
      " Training: Loss:0.13017, pen_acc:0.92310 \n",
      " Validation: Loss:1.92419, pen_acc:0.57573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [06:17<00:00, 42.54it/s]\n",
      "100%|████████████████████████████████████████| 569/569 [00:03<00:00, 165.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/18 \n",
      " Training: Loss:0.13043, pen_acc:0.92359 \n",
      " Validation: Loss:1.38211, pen_acc:0.58276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fit(model, train_dataloader, val_dataloader, optimizer, criterion, 15)\n",
    "\n",
    "fit(model, train_dataloader, val_dataloader, optimizer, criterion, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41b89c",
   "metadata": {},
   "source": [
    "#### Так как я определил первоначальную задачу, как классификацию постов по темам, а не сообществ, то и метрика каечства во время обучения считаоась по постам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-elder",
   "metadata": {},
   "source": [
    "### Подсчет метрики качества на валидационной выборке по сообществам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "concerned-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем лучшую модель по валидации\n",
    "model = torch.load('best_model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "alert-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 569/569 [00:00<00:00, 608.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#Высчитываем предсказанные метки\n",
    "model.eval()\n",
    "v_loss, v_metric, v_pred, v_lbl = valid_epoch(model, device, val_dataloader, criterion, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8acd1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6947/4078930363.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val['pred_idx'] = v_pred\n",
      "/tmp/ipykernel_6947/4078930363.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val['label_idx'] = v_lbl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>nuniq_cats</th>\n",
       "      <th>pred_idx</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>800495</td>\n",
       "      <td>motosport</td>\n",
       "      <td>[все, мы, знаем, выражение, готовь, сани, лето...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>653950686</td>\n",
       "      <td>esport</td>\n",
       "      <td>[гвардиола, об, 1, 0, с, атлетико, очень, равн...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>436974691</td>\n",
       "      <td>hockey</td>\n",
       "      <td>[комментарии, владимира, алистрова, и, седрика...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>212272560</td>\n",
       "      <td>athletics</td>\n",
       "      <td>[было, тяжело, но, мы, бежали, на, свет, я, на...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>884419658</td>\n",
       "      <td>tennis</td>\n",
       "      <td>[осака, победила, на, старте, турнира, в, сан,...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38697</th>\n",
       "      <td>653950686</td>\n",
       "      <td>esport</td>\n",
       "      <td>[️александр, кокорин, пропустит, матч, со, спе...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38700</th>\n",
       "      <td>551674788</td>\n",
       "      <td>motosport</td>\n",
       "      <td>[поздравляем, с, днем, рождения, железного, че...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38706</th>\n",
       "      <td>678833175</td>\n",
       "      <td>basketball</td>\n",
       "      <td>[берем, зайона, в, команду, если, он, похудеет...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38728</th>\n",
       "      <td>642200097</td>\n",
       "      <td>martial_arts</td>\n",
       "      <td>[️рамазан, гасанов, 8, 0, побеждает, насимжона...</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38731</th>\n",
       "      <td>362987775</td>\n",
       "      <td>extreme</td>\n",
       "      <td>[и, смотрел, он, на, безенгийскую, вязь, с, ее...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5685 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             oid      category  \\\n",
       "12        800495     motosport   \n",
       "14     653950686        esport   \n",
       "17     436974691        hockey   \n",
       "26     212272560     athletics   \n",
       "29     884419658        tennis   \n",
       "...          ...           ...   \n",
       "38697  653950686        esport   \n",
       "38700  551674788     motosport   \n",
       "38706  678833175    basketball   \n",
       "38728  642200097  martial_arts   \n",
       "38731  362987775       extreme   \n",
       "\n",
       "                                                    text  nuniq_cats  \\\n",
       "12     [все, мы, знаем, выражение, готовь, сани, лето...           1   \n",
       "14     [гвардиола, об, 1, 0, с, атлетико, очень, равн...           1   \n",
       "17     [комментарии, владимира, алистрова, и, седрика...           1   \n",
       "26     [было, тяжело, но, мы, бежали, на, свет, я, на...           1   \n",
       "29     [осака, победила, на, старте, турнира, в, сан,...           1   \n",
       "...                                                  ...         ...   \n",
       "38697  [️александр, кокорин, пропустит, матч, со, спе...           1   \n",
       "38700  [поздравляем, с, днем, рождения, железного, че...           1   \n",
       "38706  [берем, зайона, в, команду, если, он, похудеет...           1   \n",
       "38728  [️рамазан, гасанов, 8, 0, побеждает, насимжона...           1   \n",
       "38731  [и, смотрел, он, на, безенгийскую, вязь, с, ее...           1   \n",
       "\n",
       "       pred_idx  label_idx  \n",
       "12          7.0        7.0  \n",
       "14          2.0        5.0  \n",
       "17          2.0        4.0  \n",
       "26          0.0        6.0  \n",
       "29          9.0        9.0  \n",
       "...         ...        ...  \n",
       "38697       2.0        5.0  \n",
       "38700       7.0        7.0  \n",
       "38706       8.0        8.0  \n",
       "38728      11.0       11.0  \n",
       "38731       1.0        1.0  \n",
       "\n",
       "[5685 rows x 6 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['pred_idx'] = v_pred\n",
    "df_val['label_idx'] = v_lbl\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "competitive-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Группируем результаты по сообществам и берем моду по предсказанным меткам\n",
    "df_val_grouped = df_val.groupby('oid')[['label_idx', 'pred_idx']].agg(lambda x: x.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "503386f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговая `точность со штрафом` на валидации: penalty_accuracy = 0.965517\n"
     ]
    }
   ],
   "source": [
    "print(\"Итоговая `точность со штрафом` на валидации: penalty_accuracy = {:.6f}\".format(\n",
    "    penalty_accuracy(np.array(df_val_grouped.label_idx.values), \n",
    "                     np.array(df_val_grouped.pred_idx.values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174bd3e",
   "metadata": {},
   "source": [
    "### Возможности оценить качество на тестовой выборки нет, так как для нее нет меток в открытом доступе"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
